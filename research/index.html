---
layout: default
title: anh &rarr; research
---


<h1>Research Publications</h1>

<div>
    <p>My Google Scholar profile can be found
        <a href="http://scholar.google.com/citations?user=K9DwIWQAAAAJ">here</a>.
    </p>
</div>


<hr style="margin-bottom: 2em">

<div>

    <div class="section">

        <h1>2016</h1>

        <p>María Mejía, Anh Tran, and Paul Cohen.
            "<b>Human Activity Recognition Using Symbolic Sequences</b>."
            <i>ARPN Journal of Engineering and Applied Sciences</i> 11.21 (2016): 12571-12581.
            <br/>
            <b>[&nbsp;<a href>abstract</a> |
                <a href="{{ site.url }}/data/documents/papers/arpn2016-symbolic.pdf">pdf</a> |
                <a href="{{ site.url }}/data/documents/papers/arpn2016-symbolic.bib">bib</a>
                ]</b>
        </p>
        <div class="abstract">
            <span class="title">Abstract:</span>
            Human activity recognition research is an active area in an early stage of development. We present
            two approaches to activity recognition based on symbolic representations of multivariate time series
            of joint locations in articulated skeletons. One approach uses pairwise alignment and nearest-neighbour
            classification, and the other uses spectrum kernels and SVMs as classifiers. We tested both approaches
            on three datasets derived from RGBD cameras (e.g., Microsoft Kinect) as well as ordinary video, and
            compared our results with those of other researchers.
        </div>

    </div>

    <div class="section">

        <h1>2014</h1>

        <p>Anh Tran.
           "<b>Identifying Latent Attributes from Video Scenes Using Knowledge Acquired From Large Collections of Text Documents</b>."
            <i>Doctoral Dissertation</i>.
            University of Arizona.
            2014.
            <br/>
            <b>[&nbsp;<a href>abstract</a> |
                *<a href="{{ site.url }}/data/documents/papers/dissertation.pdf">pdf</a> |
                <a href="{{ site.url }}/data/documents/papers/dissertation.bib">bib</a> |
                <a href="https://github.com/trananh/vlsa/releases/tag/vlsa-1.0.2">code</a> |
                <a href>slides</a>
                <span style="display: none;" class="slides">
                    (<a href="{{ site.url }}/data/documents/talks/defense.pptx">pptx</a>)
                    (<a href="{{ site.url }}/data/documents/talks/defense.pdf">pdf</a>)
                </span>
                ]</b>
        </p>
        <div class="abstract">
            <span class="title">Abstract:</span>
            Peter Drucker, a well-known influential writer and philosopher in the field of management theory
            and practice, once claimed that "the most important thing in communication is hearing what isn't
            said." It is not difficult to see that a similar concept also holds in the context of video scene
            understanding. In almost every non-trivial video scene, most important elements, such as the motives
            and intentions of the actors, can never be seen or directly observed, yet the identification of
            these latent attributes is crucial to our full understanding of the scene. That is to say,
            latent attributes matter.
            <br/> <br/>
            In this work, we explore the task of identifying latent attributes in video scenes, focusing on
            the mental states of participant actors. We propose a novel approach to the problem based on the use of
            large text collections as background knowledge and minimal information about the videos, such as
            activity and actor types, as query context. We formalize the task and a measure of merit that
            accounts for the semantic relatedness of mental state terms, as well as their distribution weights.
            We develop and test several largely unsupervised information extraction models that identify the
            mental state labels of human participants in video scenes given some contextual information about
            the scenes. We show that these models produce complementary information and their combination
            significantly outperforms the individual models, and improves performance over several baseline
            methods on two different datasets. We present an extensive analysis of our models and close with
            a discussion of our findings, along with a roadmap for future research.
        </div>


        <p>Anh Tran, Mihai Surdeanu, and Paul Cohen.
            "<b>Extracting Latent Attributes from Video Scenes Using Text as Background Knowledge</b>."
            In <i>Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014)</i>.
            2014.
            <br/>
            <b>[&nbsp;<a href>abstract</a> |
                <a href="{{ site.url }}/data/documents/papers/starsem2014-vlsa.pdf">pdf</a> |
                <a href="{{ site.url }}/data/documents/papers/starsem2014-vlsa.bib">bib</a> |
                <a href="https://github.com/trananh/vlsa/releases/tag/vlsa-1.0.1">code</a> |
                <a href>slides</a>
                <span style="display: none;" class="slides">
                    (<a href="{{ site.url }}/data/documents/talks/starsem2014.pptx">pptx</a>)
                    (<a href="{{ site.url }}/data/documents/talks/starsem2014.pdf">pdf</a>)
                </span>
                ]</b>
        </p>
        <div class="abstract">
            <span class="title">Abstract:</span>
            We explore the novel task of identifying latent attributes in video scenes, such as the mental
            states of actors, using only large text collections as background knowledge and minimal information
            about the videos, such as activity and actor types. We formalize the task and a measure of merit that
            accounts for the semantic relatedness of mental state terms. We develop and test several largely
            unsupervised information extraction models that identify the mental states of human participants in
            video scenes. We show that these models produce complementary information and their combination
            significantly outperforms the individual models as well as other baseline methods.
        </div>


        <p>Anh Tran, Jinyan Guan, Thanima Pilantanakitti, and Paul Cohen.
            "<b>Action Recognition in the Frequency Domain</b>."
            <i>arXiv.org</i>.
            2014.
            <br/>
            <b>[&nbsp;<a href>abstract</a> |
                <a href="{{ site.url }}/data/documents/papers/arxiv2014-frequency.pdf">pdf</a> |
                <a href="{{ site.url }}/data/documents/papers/arxiv2014-frequency.bib">bib</a> |
                <a href="https://code.google.com/p/ua-gesture/">code</a>&nbsp;]</b>
        </p>
        <div class="abstract">
            <span class="title">Abstract:</span>
            In this paper, we describe a simple strategy for mitigating variability in temporal data series by
            shifting focus onto long-term, frequency domain features that are less susceptible to variability.
            We apply this method to the human action recognition task and demonstrate how working in the frequency
            domain can yield good recognition features for commonly used optical flow and articulated pose features,
            which are highly sensitive to small differences in motion, viewpoint, dynamic backgrounds, occlusion
            and other sources of variability.  We show how these frequency-based features can be used in
            combination with a simple forest classifier to achieve good and robust results on the popular KTH
            Actions dataset.
        </div>

    </div>



    <div class="section">

        <h1>2011</h1>

        <p>Wesley Kerr, Anh Tran, and Paul Cohen.
            "<b>Activity Recognition with Finite State Machines</b>."
            In <i>Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence (IJCAI'11)</i>.
            2011.
            <br/>
            <b>[&nbsp;<a href>abstract</a> |
                <a href="{{ site.url }}/data/documents/papers/ijcai2011-fluents.pdf">pdf</a> |
                <a href="{{ site.url }}/data/documents/papers/ijcai2011-fluents.bib">bib</a> |
                <a href="https://code.google.com/p/ua-time-series/">code</a>&nbsp;]</b>
        </p>
        <div class="abstract">
            <span class="title">Abstract:</span>
            This paper shows how to learn general, Finite State Machine representations of activities that
            function as recognizers of previously unseen instances of activities. The central problem is to tell
            which differences between instances of activities are unimportant and may be safely ignored for the
            purpose of learning generalized representations of activities. We develop a novel way to find the
            "essential parts" of activities by a greedy kind of multiple sequence alignment, and a method to
            transform the resulting alignments into Finite State Machine that will accept novel instances of
            activities with high accuracy.
        </div>


        <p>Tasneem Kaochar, Raquel Torres Peralta, Clayton T Morrison, Thomas J Walsh, Ian R Fasel, Sumin Beyon,
            Anh Tran, Jeremy Wright, and Paul R Cohen.
            "<b>Human Natural Instruction of a Simulated Electronic Student</b>."
            <i>AAAI Spring Symposium: Help Me Help You: Bridging the Gaps in Human-Agent Collaboration</i>.
            2011.
            <br/>
            <b>[&nbsp;<a href>abstract</a> |
                <a href="{{ site.url }}/data/documents/papers/aaai2011-workshop.pdf">pdf</a> |
                <a href="{{ site.url }}/data/documents/papers/aaai2011-workshop.bib">bib</a>&nbsp;]</b>
        </p>
        <div class="abstract">
            <span class="title">Abstract:</span>
            Humans naturally use multiple modes of instruction while teaching one another. We would like our robots
            and artificial agents to be instructed in the same way, rather than programmed. In this paper, we review
            prior work on human instruction of autonomous agents and present observations from two exploratory
            pilot studies and the results of a full study investigating how multiple instruction modes are used
            by humans. We describe our Bootstrapped Learning User Interface, a prototype multi-instruction
            interface informed by our human-user studies.
        </div>
    </div>



    <div class="section">

        <h1>2009</h1>

        <p>Jianqiang Shen, Jed Irvine, Xinlong Bao, Michael Goodman, Stephen Kolibaba, Anh Tran, Fredric Carl,
            Brenton Kirschner, Simone Stumpf, and Thomas G Dietterich.
            "<b>Detecting and Correcting User Activity Switches: Algorithms and Interfaces</b>."
            In <i>Proceedings of the International Conference on Intelligent User Interfaces (IUI'09)</i>.
            2009.
            <br/>
            <b>[&nbsp;<a href>abstract</a> |
                <a href="{{ site.url }}/data/documents/papers/iui2009-tasktracer.pdf">pdf</a> |
                <a href="{{ site.url }}/data/documents/papers/iui2009-tasktracer.bib">bib</a> |
                <a href="http://tasktracer.osuosl.org/">code</a>&nbsp;]</b>
        </p>
        <div class="abstract">
            <span class="title">Abstract:</span>
            The TaskTracer system allows knowledge workers to define a set of activities that characterize their
            desktop work. It then associates with each user-defined activity the set of resources that the user
            accesses when performing that activity. In order to correctly associate resources with activities
            and provide useful activity-related services to the user, the system needs to know the current
            activity of the user at all times. It is often convenient for the user to explicitly declare which
            activity he/she is working on. But frequently the user forgets to do this. TaskTracer applies
            machine learning methods to detect undeclared activity switches and predict the correct activity
            of the user. This paper presents TaskPredictor2, a complete redesign of the activity predictor in
            TaskTracer and its notification user interface. TaskPredictor2 applies a novel online learning
            algorithm that is able to incorporate a richer set of features than our previous predictors. We
            prove an error bound for the algorithm and present experimental results that show improved accuracy
            and a 180-fold speedup on real user data. The user interface supports negotiated interruption and
            makes it easy for the user to correct both the predicted time of the task switch and the predicted
            activity.
        </div>



        <br/>
        <p style="text-align: right; font-size: 80%"><b>*</b> preprint version</p>
    </div>


</div>


<script type="text/javascript">
//<![CDATA[
$(document).ready(function(){

		$("a:contains('abstract')").click(function(event) {
			$(this).parent().parent().nextAll(".abstract").first().slideToggle("fast"); });

		$("a:contains('abstract')").toggle(
			function(){ $(this).text("hide abstract") },
			function(){ $(this).text("abstract") }
		);

		$("a:contains('slides')").click(function(event) {
			$(this).nextAll(".slides").first().toggle(); });
	    $("a:contains('slides')").toggle(
			function(){ $(this).text("slides: ") },
			function(){ $(this).text("slides") }
		);
});
//]]>
</script>
