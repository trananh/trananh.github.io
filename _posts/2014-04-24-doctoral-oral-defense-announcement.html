---
layout: post
title: Doctoral Oral Defense Announcement

location: Tucson, AZ
excerpt: Please join me on April 30th, 2014 for my doctoral oral defense.
category: work

comments: true
---

<span itemscope itemtype="http://schema.org/Event">

<div class="update">
    <span class="title">Update [08.2014] - </span>
    My dissertation and defense presentation are now available for download. <br/>
    [&nbsp;<a href="{{ site.url }}/data/documents/papers/dissertation.pdf">dissertation.pdf</a> |
    <a href="{{ site.url }}/data/documents/papers/dissertation.bib">dissertation.bib</a> |
    <a href="{{ site.url }}/data/documents/talks/defense.pptx">defense.pptx</a> |
    <a href="{{ site.url }}/data/documents/talks/defense.pdf">defense.pdf</a>&nbsp;]
</div>

<p>You are cordially invited to my doctoral oral defense, which is open to the public.</p>

<table style="padding:5px">
    <tr>
        <td align="right" style="vertical-align: top; padding-right: 10px"><b>Date:</b></td>
        <td itemprop="startDate" content="2014-04-30T09:00">April 30th, 2014</td>
    </tr>
    <tr>
        <td align="right" style="vertical-align: top; padding-right: 10px"><b>Time:</b></td>
        <td>9AM - 10AM</td>
    </tr>
    <tr>
        <td align="right" style="vertical-align: top; padding-right: 10px"><b>Location:</b></td>
        <td itemprop="location" itemscope itemtype="http://schema.org/Place">
            <span itemprop="name">Gould-Simpson, Room 1027</span>
            <a target="_blank" title="Google Map" href="https://goo.gl/maps/1Yw8n" style="color: gray">
                [<i class="fa fa-map-marker"></i>]
            </a>
        </td>
    </tr>
    <tr><td>&nbsp;</td></tr>
    <tr>
        <td align="right" style="vertical-align: top; padding-right: 10px"><b>Candidate:</b></td>
        <td itemprop="performer" itemscope itemtype="http://schema.org/Person">
            <span itemprop="name">Anh Xuan Tran</span>
        </td>
    </tr>
    <tr>
        <td align="right" style="vertical-align: top; padding-right: 10px"><b>Committee:</b></td>
        <td>Paul Cohen, Mihai Surdeanu, Kobus Barnard, Ken McAllister
        </td>
    </tr>
    <tr><td>&nbsp;</td></tr>
    <tr>
        <td align="right" style="vertical-align: top; padding-right: 10px">
            <b>Title:</b>
        </td>
        <td style="font-variant: small-caps; font-weight: bold">Identifying Latent Attributes from Video Scenes Using Knowledge Acquired
            From Large Collections of Text Documents
        </td>
    </tr>
    <tr><td>&nbsp;</td></tr>
    <tr>
        <td align="right" style="vertical-align: top; padding-right: 10px">
            <b>Abstract:</b>
        </td>
        <td style="font-size: 0.875em">

            <p>Peter Drucker, a well-known influential writer and philosopher in the field of management theory
                and practice, once claimed that "the most important thing in communication is hearing what isn't
                said." It is not difficult to see that a similar concept also holds in the context of video scene
                understanding. In almost every non-trivial video scene, most important elements, such as the motives
                and intentions of the actors, can never be seen or directly observed, yet the identification of
                these latent attributes is crucial to our full understanding of the scene. That is to say,
                latent attributes matter.
            </p>
            <p>In this work, we explore the task of identifying latent attributes in video scenes, focusing on
                the mental states of participant actors. We propose a novel approach to the problem based on the use of
                large text collections as background knowledge and minimal information about the videos, such as
                activity and actor types, as query context. We formalize the task and a measure of merit that
                accounts for the semantic relatedness of mental state terms, as well as their distribution weights.
                We develop and test several largely unsupervised information extraction models that identify the
                mental state labels of human participants in video scenes given some contextual information about
                the scenes. We show that these models produce complementary information and their combination
                significantly outperforms the individual models, and improves performance over several baseline
                methods on two different datasets. We present an extensive analysis of our models and close with
                a discussion of our findings, along with a roadmap for future research.
            </p>
        </td>
    </tr>

</table>

<meta itemprop="url" content="{{ page.url }}">
<meta itemprop="description" content="Title: Identifying Latent Attributes from Video Scenes Using Knowledge Acquired from Large Collections of Text Documents">

</span>